{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Vector spaces and embeddings in NLP and LLMs</a>\n",
    "\n",
    " 1. <a href=\"#1\">LLM Embeddings as an application of vector spaces</a> \n",
    " 2. <a href=\"#2\">Distance and similarity in vector space</a>\n",
    " 3. <a href=\"#3\">Analogy reasoning</a>\n",
    "\n",
    "Vector spaces are a core concept in linear algebra, with applications in various machine learning algorithms. In this notebook, we will explore the definition, properties, and applications of vector spaces. We will use Large Language Model (LLM) embeddings as a practical example to illustrate these concepts.\n",
    "\n",
    "A vector space is a collection of objects, called vectors, that can be added together and multiplied by scalars. Formally, a vector space $V$ over a field $F$ is a set equipped with two operations:\n",
    "\n",
    "1. **Vector Addition**: For any vectors $\\mathbf{u}, \\mathbf{v} \\in V$, there exists a vector $\\mathbf{u} + \\mathbf{v} \\in V$.\n",
    "2. **Scalar Multiplication**: For any scalar $c \\in F$ and vector $\\mathbf{v} \\in V$, there exists a vector $c\\mathbf{v} \\in V$.\n",
    "\n",
    "These operations must satisfy certain axioms, such as associativity, commutativity, and distributivity.\n",
    "\n",
    "Vector spaces have several important properties:\n",
    "\n",
    "1. **Closure**: The sum of any two vectors in the space is also in the space.\n",
    "2. **Zero Vector**: There exists a zero vector $\\mathbf{0}$ such that $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ for any vector $\\mathbf{v}$.\n",
    "3. **Inverse**: For every vector $\\mathbf{v}$, there exists a vector $-\\mathbf{v}$ such that $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$.\n",
    "4. **Scalar Multiplication**: The product of any scalar and any vector is also in the space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. LLM Embeddings as an application of vector spaces</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Many NLP constructs, such as Large Language Models (LLMs), are based on the concept of **embeddings**, which are vector representations of words, sentences, or documents. These embeddings live in a high-dimensional vector space, where each dimension captures some aspect of the meaning or context of the text.\n",
    "\n",
    "For example, consider a simple LLM that generates embeddings for words. Each word is represented as a vector in a high-dimensional space, where the dimensions correspond to different semantic features. These embeddings can be added, subtracted, and scaled, making the space of embeddings a vector space.\n",
    "\n",
    "Which [Amazon Bedrock](https://aws.amazon.com/bedrock/), we have access to various embedding models, for instance the [Amazon Titan embedding models](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html). [This article](https://aws.amazon.com/jp/blogs/machine-learning/get-started-with-amazon-titan-text-embeddings-v2-a-new-state-of-the-art-embeddings-model-on-amazon-bedrock/) explains in detail what the state-of-the-art Titan models are capable of. We will use Amazon Titan Text Embeddings V2 in the demo below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client using boto3\n",
    "bedrock = boto3.client(\"bedrock\")\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's an example of the embedding produced by Amazon Titan Text Embeddings V2 on a word. The embedding dimension, i.e. the length of the embedding vector, is a characteristic of every embedding model and is typically on the order of magnitude of the thousands for current state-of-the-art models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector space has 1024 dimensions\n",
      "\n",
      "Printing the first 10 components of the embedding vector for word 'amazing':\n",
      "\n",
      "[0.009552421979606152, 0.017132731154561043, -0.04338648542761803, -0.035005006939172745, 0.031553808599710464, 0.07050304114818573, -0.014359447173774242, -0.028102610260248184, -0.007364609278738499, -0.016023417934775352]\n"
     ]
    }
   ],
   "source": [
    "# Define the word for which we want to generate an embedding\n",
    "word = \"amazing\"\n",
    "payload = {\"inputText\": word}\n",
    "\n",
    "# Invoke the model to generate the embedding\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "    body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Parse the response body from the model invocation\n",
    "body = json.loads(response.get('body').read())\n",
    "\n",
    "print(f\"The vector space has {len(body['embedding'])} dimensions\\n\")\n",
    "\n",
    "# Printing the first 10 components of the embedding vector\n",
    "print(f\"Printing the first 10 components of the embedding vector for word '{word}':\\n\\n{body['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Distance and similarity in vector space</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Vectors are often depicted as arrows pointing in a certain direction within a space. They can also be represented numerically through the list of their components on a basis, which makes mathematical operations easier.\n",
    "\n",
    "The purpose of vector embeddings is to map complex entities like text or images into a vector space, allowing machine learning models to identify patterns and relationships. These embeddings convert non-numeric data into a numerical format while preserving their semantic meanings and relationships. This enables computational models in machine learning and natural language processing (NLP) to recognize similarities and differences between entities.\n",
    "\n",
    "In a vector space embedding, similar entities are placed close to each other, reflecting their semantic or contextual similarity. For example, in word embeddings, words with similar meanings are positioned near each other in the vector space. This spatial arrangement helps embeddings capture and organize the semantic relationships between entities, a concept known as the semantic space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's create a function that returns the embedding given a text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute embeddings from text input\n",
    "def get_embedding(text, model=\"amazon.titan-embed-text-v2:0\"):\n",
    "    payload = {\"inputText\": text}\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model,\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    body = json.loads(response.get(\"body\").read())\n",
    "    \n",
    "    # Return np array as it's easier to deal with\n",
    "    return np.array(body[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python type of the embedding vector is: <class 'numpy.ndarray'>\n",
      "The array shape is: emb1.shape=(1024,)\n"
     ]
    }
   ],
   "source": [
    "# testing it\n",
    "emb1 = get_embedding(\"cat\")\n",
    "print(f\"The Python type of the embedding vector is: {type(emb1)}\\nThe array shape is: {emb1.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "In the context of embeddings, semantically similar strings (words, phrases, or sentences) are represented by vectors that are \"close\" to each other in the embedding space. This concept is fundamental to many natural language processing (NLP) tasks, such as word similarity, sentence similarity, and clustering. The concept of \"closeness\" in the embedding space can be measured using various distance metrics, with Euclidean distance and cosine similarity being two of the most commonly used methods.\n",
    "\n",
    "* **Euclidean distance** measures the straight-line distance between two points in the embedding space. A small value of the distance between two words embeddings means that the words are close to each other.\n",
    "* **Cosine similarity** measures the cosine of the angle between two vectors. It is particularly useful for high-dimensional spaces because it focuses on the orientation of the vectors rather than their magnitude, and it is bounded between -1 (diametrically opposed vectors) to 1 (parallel vectors). When two word embeddings are close to each other, the cosine similarity is large. \n",
    "\n",
    "I.e. words or sentences that are close will have: \n",
    "- small Euclidean distance\n",
    "- large cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's create two functions that return the above metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    # Calculate the dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "\n",
    "    # Calculate the norms of the vectors\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    cosine_sim = dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    # Calculate the difference between the vectors\n",
    "    difference = vector1 - vector2\n",
    "\n",
    "    # Calculate the Euclidean distance\n",
    "    euclidean_dist = np.linalg.norm(difference)\n",
    "\n",
    "    return euclidean_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's test them for concepts that are semantically approaching each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<img src=\"../../images/vector_similarity.png\" alt=\"vector_similarity\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>euclidean_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mouse</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.197041</td>\n",
       "      <td>1.267248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pizza</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.285829</td>\n",
       "      <td>1.195133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rome</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.407268</td>\n",
       "      <td>1.088790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.474605</td>\n",
       "      <td>1.025081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italy</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.908031</td>\n",
       "      <td>0.428879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>italy</td>\n",
       "      <td>italy</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_1 word_2  cosine_similarity  euclidean_distance\n",
       "0    mouse  italy           0.197041            1.267248\n",
       "1    pizza  italy           0.285829            1.195133\n",
       "2     rome  italy           0.407268            1.088790\n",
       "3  italian  italy           0.474605            1.025081\n",
       "4    Italy  italy           0.908031            0.428879\n",
       "5    italy  italy           1.000000            0.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"mouse\", \"pizza\", \"rome\", \"italian\", \"Italy\", \"italy\"]\n",
    "base_word = \"italy\"\n",
    "\n",
    "# Construct dataframe to store data\n",
    "df = pd.DataFrame(words, columns=[\"word_1\"])\n",
    "df[\"word_2\"] = base_word\n",
    "\n",
    "# Compute distance metrics for each pair of words\n",
    "df[\"cosine_similarity\"] = df.apply(lambda r: cosine_similarity(get_embedding(r.word_1), get_embedding(r.word_2)), axis=1)\n",
    "df[\"euclidean_distance\"] = df.apply(lambda r: euclidean_distance(get_embedding(r.word_1), get_embedding(r.word_2)), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's repeat the same exercise. This time, instead of just single words, we use sentences. Let's see how the Cosine similarity and Euclidean distance compare when used to compare a human (positive) review with:\n",
    "* a similar review generated by an LLM\n",
    "* a negative review \n",
    "* an irrelevant review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>euclidean_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Featuring a slim design, this laptop offers a ...</td>\n",
       "      <td>this desktop is a top performing one and it is...</td>\n",
       "      <td>0.217083</td>\n",
       "      <td>1.251333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The desktop lacks in battery duration, its dis...</td>\n",
       "      <td>this desktop is a top performing one and it is...</td>\n",
       "      <td>0.351024</td>\n",
       "      <td>1.139277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was a sunny and crisp day in the village lo...</td>\n",
       "      <td>this desktop is a top performing one and it is...</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>1.382016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence_1  \\\n",
       "0  Featuring a slim design, this laptop offers a ...   \n",
       "1  The desktop lacks in battery duration, its dis...   \n",
       "2  It was a sunny and crisp day in the village lo...   \n",
       "\n",
       "                                          sentence_2  cosine_similarity  \\\n",
       "0  this desktop is a top performing one and it is...           0.217083   \n",
       "1  this desktop is a top performing one and it is...           0.351024   \n",
       "2  this desktop is a top performing one and it is...           0.045016   \n",
       "\n",
       "   euclidean_distance  \n",
       "0            1.251333  \n",
       "1            1.139277  \n",
       "2            1.382016  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_description = \"this desktop is a top performing one and it is able to provide me with good amount of vRAM and speed\"\n",
    "llm_description = \"Featuring a slim design, this laptop offers a beautiful and extended battery duration.\"\n",
    "bad_description = \"The desktop lacks in battery duration, its display resolution is less, and the refresh rate is alsoless,  appearance is bulky.\"\n",
    "irrelevant_description = \"It was a sunny and crisp day in the village located right on the bora bora coast.\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "descriptions = [llm_description, bad_description, irrelevant_description]\n",
    "base_description = human_description\n",
    "\n",
    "# Construct dataframe to store data\n",
    "df = pd.DataFrame(descriptions, columns=[\"sentence_1\"])\n",
    "df[\"sentence_2\"] = base_description\n",
    "\n",
    "# Compute distance metrics for each pair of sentences\n",
    "df[\"cosine_similarity\"] = df.apply(lambda r: cosine_similarity(get_embedding(r.sentence_1), get_embedding(r.sentence_2)), axis=1)\n",
    "df[\"euclidean_distance\"] = df.apply(lambda r: euclidean_distance(get_embedding(r.sentence_1), get_embedding(r.sentence_2)), axis=1)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Analogy reasoning</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "As elements of a vector space, the usual operations on embeddings are well defined. By performing vector operations on embeddings, we can uncover interesting relationships and patterns within the language. \n",
    "\n",
    "One such application is **analogy reasoning** to find words that are related in a similar way to a given pair of words. For instance, in the classic example `king - man + female = queen` the embedding for \"queen\" can be approximated by subtracting the embedding for \"man\" from the embedding for \"king\" and then adding the embedding for \"female.\" This demonstrates the ability of LLM embeddings to capture subtle semantic nuances and perform complex linguistic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In the next cell, we give a demonstration of analogy reasoning. We create a function `analogy_reasoning` that performs the following steps:\n",
    "\n",
    "* Retrieves embeddings for the given query and target words.\n",
    "* Calculates an analogy vector based on the query words.\n",
    "* Computes cosine similarity and Euclidean distance between the analogy vector and each target word's embedding.\n",
    "* Identifies the closest target word based on the similarity measures.\n",
    "* The function returns a tuple containing the closest target word based on both cosine similarity and Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analogy_reasoning(query_words, target_words, get_embedding_func):\n",
    "    \"\"\"\n",
    "    Performs analogy reasoning using LLM embeddings.\n",
    "\n",
    "    Args:\n",
    "    query_words: A list of query words for the analogy.\n",
    "    target_words: A list of target words to compare.\n",
    "    get_embedding_func: A function to get the embedding for a word.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the closest target word based on cosine similarity and Euclidean distance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get embeddings for query and target words\n",
    "    query_embs = np.array([get_embedding_func(word) for word in query_words])\n",
    "    target_embs = np.array([get_embedding_func(word) for word in target_words])\n",
    "\n",
    "    # Calculate analogy vector\n",
    "    analogy_vec = query_embs[0] - query_embs[1] + query_embs[2]\n",
    "\n",
    "    # Calculate cosine similarity and Euclidean distance\n",
    "    cos_sim = np.array([cosine_similarity(target, analogy_vec) for target in target_embs])\n",
    "    euc_dist = np.array([euclidean_distance(target, analogy_vec) for target in target_embs])\n",
    "\n",
    "    # Find closest target based on cosine similarity and Euclidean distance\n",
    "    closest_index_cos = np.argmax(cos_sim)\n",
    "    closest_index_dist = np.argmin(euc_dist)\n",
    "    \n",
    "    print(f\"Cosine similarity   |  {query_words[0]} - {query_words[1]} + {query_words[2]} ~ {target_words[closest_index_cos]}\")\n",
    "    print(f\"Euclidean distance  |  {query_words[0]} - {query_words[1]} + {query_words[2]} ~ {target_words[closest_index_dist]}\")\n",
    "\n",
    "    return target_words[closest_index_cos], target_words[closest_index_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Trying to find the closest word embedding from a whole vocabulary would be very costly. Thus, we will choose a handful of words as targets and let the `analogy_reasoning` function find the closest among that set. \n",
    "\n",
    "Let's first check that the `king - man + female ~ queen` analogy holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity   |  king - man + female ~ queen\n",
      "Euclidean distance  |  king - man + female ~ queen\n"
     ]
    }
   ],
   "source": [
    "query_words = ['king', 'man', 'female']\n",
    "target_words = [\"queen\", \"princess\", \"duchess\", \"prince\"]\n",
    "\n",
    "result = analogy_reasoning(query_words, target_words, get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries and capitals\n",
      "Cosine similarity   |  Dublin - Ireland + Madrid ~ Spain\n",
      "Euclidean distance  |  Dublin - Ireland + Madrid ~ Spain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Countries and capitals\n",
    "query_words = [\"Dublin\", \"Ireland\", \"Madrid\"]\n",
    "target_words = [\"Italy\", \"Spain\", \"Germany\", \"Norway\", \"Portugal\", \"United Kingdom\"]\n",
    "\n",
    "print(\"Countries and capitals\")\n",
    "result = analogy_reasoning(query_words, target_words, get_embedding)\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebc266-e00b-4284-97ba-6a8508e2eadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
